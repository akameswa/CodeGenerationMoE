{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPNPie5Eo3EZ",
        "outputId": "d907812a-cca1-473c-f776-fe86a2baeda6"
      },
      "outputs": [],
      "source": [
        "!git clone -b mixtral https://github.com/cg123/mergekit.git\n",
        "!cd mergekit && pip install -qqq -e . --progress-bar off\n",
        "!pip install -qqq -U transformers --progress-bar off\n",
        "!pip install -qqq bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGd7jlfCpNcg"
      },
      "outputs": [],
      "source": [
        "merge_config  = \"\"\"\n",
        "base_model: unsloth/mistral-7b-instruct-v0.2-bnb-4bit\n",
        "gate_mode: hidden\n",
        "dtype: float16\n",
        "experts:\n",
        "  - source_model: akameswa/mistral-7b-instruct-javascript-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at javascript\"]\n",
        "  - source_model: akameswa/mistral-7b-instruct-java-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at java\"]\n",
        "  - source_model: akameswa/mistral-7b-instruct-cpp-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at cpp\"]\n",
        "  - source_model: akameswa/mistral-7b-instruct-python-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at python\"]\n",
        "\"\"\"\n",
        "\n",
        "with open('config.yaml', 'w') as f:\n",
        "    f.write(merge_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5mYzDo1q96y",
        "outputId": "2c12e394-7f1b-408d-e65c-2906bf84c856"
      },
      "outputs": [],
      "source": [
        "!mergekit-moe config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w-RNKev373lI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU huggingface_hub\n",
        "\n",
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "from jinja2 import Template\n",
        "import yaml\n",
        "\n",
        "username = \"akameswa\"\n",
        "MODEL_NAME = \"mixtral-4x7b-instruct-code\"\n",
        "template_text = \"\"\"\n",
        "---\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- mergekit\n",
        "- lazymergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a MoE of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## ðŸ§© Configuration\n",
        "\n",
        "```yaml\n",
        "{{- merge_config -}}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "jinja_template = Template(template_text.strip())\n",
        "\n",
        "data = yaml.safe_load(merge_config)\n",
        "if \"experts\" in data:\n",
        "    models = [expert[\"source_model\"] for expert in data[\"experts\"]]\n",
        "else:\n",
        "    raise Exception(\"No experts found in yaml config\")\n",
        "\n",
        "content = jinja_template.render(\n",
        "    model_name=MODEL_NAME,\n",
        "    models=models,\n",
        "    yaml_config=merge_config,\n",
        "    username=username,\n",
        ")\n",
        "\n",
        "card = ModelCard(content)\n",
        "card.save('merge/README.md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik0V0dF55gfU"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "username = \"akameswa\"\n",
        "\n",
        "api = HfApi(token=\"\")\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "api.upload_folder(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path=\"merge\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
