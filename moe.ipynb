{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPNPie5Eo3EZ",
        "outputId": "d907812a-cca1-473c-f776-fe86a2baeda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mergekit'...\n",
            "remote: Enumerating objects: 1287, done.\u001b[K\n",
            "remote: Counting objects: 100% (715/715), done.\u001b[K\n",
            "remote: Compressing objects: 100% (286/286), done.\u001b[K\n",
            "remote: Total 1287 (delta 579), reused 475 (delta 428), pack-reused 572\u001b[K\n",
            "Receiving objects: 100% (1287/1287), 365.57 KiB | 1.89 MiB/s, done.\n",
            "Resolving deltas: 100% (870/870), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for mergekit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone -b mixtral https://github.com/cg123/mergekit.git\n",
        "!cd mergekit && pip install -qqq -e . --progress-bar off\n",
        "!pip install -qqq -U transformers --progress-bar off\n",
        "!pip install -qqq bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGd7jlfCpNcg"
      },
      "outputs": [],
      "source": [
        "merge_config  = \"\"\"\n",
        "base_model: unsloth/mistral-7b-instruct-v0.2-bnb-4bit\n",
        "gate_mode: hidden\n",
        "dtype: float16\n",
        "experts:\n",
        "  - source_model: akameswa/mistral-7b-instruct-javascript-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at javascript\"]\n",
        "  - source_model: akameswa/mistral-7b-instruct-java-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at java\"]\n",
        "  - source_model: akameswa/mistral-7b-instruct-cpp-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at cpp\"]\n",
        "  - source_model: akameswa/mistral-7b-instruct-python-4bit\n",
        "    positive_prompts: [\"You are helpful a coding assistant good at python\"]\n",
        "\"\"\"\n",
        "\n",
        "with open('config.yaml', 'w') as f:\n",
        "    f.write(merge_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5mYzDo1q96y",
        "outputId": "2c12e394-7f1b-408d-e65c-2906bf84c856"
      },
      "outputs": [],
      "source": [
        "!mergekit-moe config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w-RNKev373lI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU huggingface_hub\n",
        "\n",
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "from jinja2 import Template\n",
        "import yaml\n",
        "\n",
        "username = \"akameswa\"\n",
        "MODEL_NAME = \"mixtral-4x7b-instruct-code\"\n",
        "template_text = \"\"\"\n",
        "---\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- mergekit\n",
        "- lazymergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a MoE of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## ðŸ§© Configuration\n",
        "\n",
        "```yaml\n",
        "{{- merge_config -}}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "jinja_template = Template(template_text.strip())\n",
        "\n",
        "data = yaml.safe_load(merge_config)\n",
        "if \"experts\" in data:\n",
        "    models = [expert[\"source_model\"] for expert in data[\"experts\"]]\n",
        "else:\n",
        "    raise Exception(\"No experts found in yaml config\")\n",
        "\n",
        "content = jinja_template.render(\n",
        "    model_name=MODEL_NAME,\n",
        "    models=models,\n",
        "    yaml_config=merge_config,\n",
        "    username=username,\n",
        ")\n",
        "\n",
        "card = ModelCard(content)\n",
        "card.save('merge/README.md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik0V0dF55gfU"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "username = \"akameswa\"\n",
        "\n",
        "api = HfApi(token=\"\")\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "api.upload_folder(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path=\"merge\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
